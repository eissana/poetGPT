{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "text:  چو ترکان بدیدند کارجاسپ رفت\tهمی آید از هر سوی تیغ تفت\n",
      "\n",
      "tokens: [' ', 'چو ', 'ترکان ب', 'دیدند ', 'کارج', 'اسپ ', 'رفت', '\\tهمی ', 'آید از هر سوی ', 'تیغ ', 'تفت']\n",
      "\n",
      "token ids: [7, 111, 2951, 2165, 29842, 515, 157, 294, 15461, 716, 27223]\n",
      "\n",
      "decoded text:   چو  ترکان ب دیدند  کارج اسپ  رفت \tهمی  آید از هر سوی  تیغ  تفت\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tk = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "# tk.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tk.enable_padding(pad_id=3)\n",
    "\n",
    "files = [\"data/shahname_src.txt\", \"data/shahname_tgt.txt\"]\n",
    "tk.train(files, trainer)\n",
    "\n",
    "text = \" چو ترکان بدیدند کارجاسپ رفت\tهمی آید از هر سوی تیغ تفت\"\n",
    "print(f\"text: {text}\")\n",
    "\n",
    "encoded = tk.encode(text)\n",
    "print(f\"\\ntokens: {encoded.tokens}\")\n",
    "print(f\"\\ntoken ids: {encoded.ids}\")\n",
    "\n",
    "decoded = tk.decode(encoded.ids)\n",
    "print(f\"\\ndecoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 111, 2951, 2165, 29842, 515, 157, 294, 15461, 716, 27223]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tk.encode_batch([text])\n",
    "[enc.ids for enc in encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1105, token: تیغ\n",
      "['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]', '\\t', '\\n', ' ', '(', ')', '«', '»', '،', '؟', 'ء', 'آ', 'أ', 'ؤ', 'ئ', 'ا']\n"
     ]
    }
   ],
   "source": [
    "token = \"تیغ\"\n",
    "id = tk.token_to_id(token)\n",
    "print(f\"id: {id}, token: {tk.id_to_token(id)}\")\n",
    "print([tk.id_to_token(i) for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4460, -1.1201, -0.9348, -1.4863,  0.9522],\n",
       "        [-0.4075, -1.2803,  0.8713, -3.7395,  1.7959],\n",
       "        [ 1.7201, -0.3779, -0.7992, -2.0305, -0.4960],\n",
       "        [ 1.5586,  1.2785, -0.2851,  0.2826,  0.4734],\n",
       "        [-1.0473, -1.5379, -2.1796,  1.1212,  0.3986]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.randn(5, 5)\n",
    "m, n = weights.shape\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4460,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.4075, -1.2803,    -inf,    -inf,    -inf],\n",
       "        [ 1.7201, -0.3779, -0.7992,    -inf,    -inf],\n",
       "        [ 1.5586,  1.2785, -0.2851,  0.2826,    -inf],\n",
       "        [-1.0473, -1.5379, -2.1796,  1.1212,  0.3986]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.masked_fill(torch.tril(torch.ones(m, n)) == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, train_ratio=0.8, val_ratio=0.1):\n",
    "    src, tgt = data\n",
    "    assert len(src) == len(tgt), \"expeted the same source and target sizes.\"\n",
    "\n",
    "    ntrain = int(train_ratio * len(src))\n",
    "    nval = int(val_ratio * len(src))\n",
    "    indices = torch.randperm(len(src))\n",
    "\n",
    "    train = (src[indices][:ntrain], tgt[indices][:ntrain])\n",
    "    val = (src[indices][ntrain : ntrain + nval], tgt[indices][ntrain : ntrain + nval])\n",
    "    test = (src[indices][ntrain + nval :], tgt[indices][ntrain + nval :])\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def get_batch(data, batch_size, device):\n",
    "    \"\"\"\n",
    "    Generates a batch of examples.\n",
    "    \"\"\"\n",
    "    src, tgt = data\n",
    "    assert len(src) == len(tgt), \"expeted the same source and target sizes.\"\n",
    "\n",
    "    indices = torch.randint(len(src), (batch_size,))\n",
    "\n",
    "    x = src[indices].to(device)\n",
    "    y = tgt[indices].to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source batch data size: torch.Size([49609, 13])\n",
      "target batch data size: torch.Size([49609, 10])\n",
      "'\tهمی  آید از هر سوی  تیغ  تفت\n",
      "'\n",
      "tensor([  294, 15461,   716,  1741,     3,     3,     3,     3,     3,     3])\n",
      "['\\tهمی ', 'آید از هر سوی ', 'تیغ ', 'تفت\\n', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/shahname_src.txt\") as f:\n",
    "    src = f.read().splitlines()\n",
    "\n",
    "src = torch.tensor([x.ids for x in tk.encode_batch(src)], dtype=torch.long)\n",
    "print(f\"source batch data size: {src.shape}\")\n",
    "\n",
    "with open(\"data/shahname_tgt.txt\") as f:\n",
    "    tgt = f.read().splitlines(keepends=True)\n",
    "\n",
    "tgt = torch.tensor([x.ids for x in tk.encode_batch(tgt)], dtype=torch.long)\n",
    "print(f\"target batch data size: {tgt.shape}\")\n",
    "\n",
    "print(\"'\" + tk.decode(tgt[:1][0].tolist()) + \"'\")\n",
    "print(tgt[:1][0])\n",
    "print([tk.id_to_token(id) for id in tgt[:1][0]])\n",
    "\n",
    "train, val, _ = split((src, tgt), 0.9, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  571, 26770,  1600,  4257,  1019,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3],\n",
       "         [10837, 11853,  1109,   521,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3]]),\n",
       " tensor([[2087, 1985,  547,  693, 1106,    3,    3,    3,    3,    3],\n",
       "         [   5,  549,  334, 5997, 5672, 1093,  554,    3,    3,    3]]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src, tgt = get_batch(train, 2, \"cpu\")\n",
    "src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['چو ترکان بدیدند کارجاسپ رفت',\n",
       "  'همه سرکشانشان پیاده شدند',\n",
       "  'کمانچای چاچی بینداختند',\n",
       "  'به زاریش گفتند گر شهریار',\n",
       "  'بدین اندر آییم و خواهش کنیم'],\n",
       " ['\\tهمی آید از هر سوی تیغ تفت\\n',\n",
       "  '\\tبه پیش گو اسفندیار آمدند\\n',\n",
       "  '\\tقبای نبردی برون آختند\\n',\n",
       "  '\\tدهد بندگان را به جان زینهار\\n',\n",
       "  '\\tهمه آذران را نیایش کنیم\\n'])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/shahname.txt\") as f:\n",
    "    lines = f.read().splitlines(keepends=True)\n",
    "\n",
    "lines = lines[:10]\n",
    "\n",
    "lines = [line.split(\"\\t\") for line in lines]\n",
    "src = [line[0] for line in lines if len(line) == 2]\n",
    "tgt = [\"\\t\"+line[1] for line in lines if len(line) == 2]\n",
    "src[:5], tgt[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['چو ترکان بدیدند کارجاسپ رفت\\tهمی آید از هر سوی تیغ تفت\\n',\n",
       " 'همه سرکشانشان پیاده شدند\\tبه پیش گو اسفندیار آمدند\\n',\n",
       " 'کمانچای چاچی بینداختند\\tقبای نبردی برون آختند\\n',\n",
       " 'به زاریش گفتند گر شهریار\\tدهد بندگان را به جان زینهار\\n',\n",
       " 'بدین اندر آییم و خواهش کنیم\\tهمه آذران را نیایش کنیم\\n',\n",
       " 'ازیشان چو بشنید اسفندیار\\tبه جان و به تن دادشان زینهار\\n',\n",
       " 'بران لشگر گشن آواز داد\\tگو نامبردار فرخ نژاد\\n',\n",
       " 'که این نامداران ایرانیان\\tبگردید زین لشکر چینیان\\n',\n",
       " 'کنون کاین سپاه عدو گشت پست\\tازین سهم و کشتن بدارید دست\\n',\n",
       " 'که بس زاروارند و بیچاره وار\\tدهدی این سگان را به جان زینهار\\n']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/shahname.txt\") as f:\n",
    "    lines = f.read().splitlines(keepends=True)\n",
    "\n",
    "src_token_ids = torch.tensor([x.ids for x in tk.encode_batch(src_lines)], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[111, 2951, 2165, 29842, 515, 157]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_mesra = \"چو ترکان بدیدند کارجاسپ رفت\"\n",
    "tk.encode(first_mesra).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.],\n",
       "        [0., 1., 1.]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "a = torch.cat([torch.zeros(a.shape[0], 1), a], dim=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "stream did not contain valid UTF-8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[225], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n\u001b[1;32m     10\u001b[0m text_file \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/hafez.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m tk \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[225], line 6\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[0;34m(filenames)\u001b[0m\n\u001b[1;32m      3\u001b[0m tk\u001b[38;5;241m.\u001b[39menable_padding(pad_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BpeTrainer(special_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SOS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[EOS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "\u001b[0;31mException\u001b[0m: stream did not contain valid UTF-8"
     ]
    }
   ],
   "source": [
    "def get_tokenizer(filenames):\n",
    "    tk = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tk.enable_padding(pad_id=3)\n",
    "\n",
    "    trainer = BpeTrainer(special_tokens=[\"[SOS]\", \"[EOS]\", \"[UNK]\", \"[PAD]\"])\n",
    "    tk.train(filenames, trainer)\n",
    "\n",
    "    return tk\n",
    "\n",
    "text_file = [\"data/hafez.txt\"]\n",
    "tk = get_tokenizer(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21808,   304,     7,     7,     0,     6,     6,  1748,     0,    71])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(text_file[0]) as f:\n",
    "    text = f.read()\n",
    "\n",
    "token_ids = torch.tensor(tk.encode(text).ids, dtype=torch.long)\n",
    "\n",
    "token_ids[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
