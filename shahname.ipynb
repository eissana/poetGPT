{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"batch_size\": 8,\n",
    "    \"embedding_dim\": 8,\n",
    "    \"nhead\": 2,\n",
    "    \"num_encoder_layers\": 2,\n",
    "    \"num_decoder_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"block_size\": 13,\n",
    "    \"dim_feedforward\": 4,\n",
    "}\n",
    "load_model = False\n",
    "save_model = True\n",
    "model_filename = \"models/shahname.pt\"\n",
    "mesra_delimiter = \"\\t\"\n",
    "beyt_delimiter = \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention head layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, head_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, v, k, q, mask=None):\n",
    "        _, _, C = q.shape\n",
    "        value, key, query = self.value(v), self.key(k), self.query(q)\n",
    "        weights = query @ key.transpose(-2, -1) * C**-0.5\n",
    "\n",
    "        if mask is not None:\n",
    "            weights = weights.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        out = weights @ value\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        assert (\n",
    "            embedding_dim % num_heads == 0\n",
    "        ), f\"{embedding_dim=} must be divisible by {num_heads=}\"\n",
    "        head_size = embedding_dim // num_heads\n",
    "\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(embedding_dim, head_size, dropout) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, v, k, q, mask=None):\n",
    "        v, k, q = self.ln(v), self.ln(k), self.ln(q)\n",
    "        out = torch.cat([head(v, k, q, mask) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, dim_feedforward * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward * embedding_dim, embedding_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ffn(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # multi-head self attention with no mask. All nodes may communicate.\n",
    "        self.attn = MultiheadAttention(embedding_dim, num_heads, dropout)\n",
    "        self.ffn = FeedForward(embedding_dim, dim_feedforward)\n",
    "\n",
    "    def forward(self, v, k, q):\n",
    "        out = q + self.attn(v, k, q)\n",
    "        out = out + self.ffn(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz, device):\n",
    "    return torch.tril(torch.ones(sz, sz).to(device)) == 0\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # multi-head self attention with triangular mask. Nodes communicate only\n",
    "        # with previous nodes.\n",
    "        self.attn = MultiheadAttention(embedding_dim, num_heads, dropout)\n",
    "        # Reusing Encoder as the top part of the decoder with a multi-head\n",
    "        # cross-attention and a feed-forward network on top of it.\n",
    "        self.attn_ffn = EncoderBlock(embedding_dim, num_heads, dim_feedforward, dropout)\n",
    "\n",
    "    def forward(self, enc_out, dec_in, mask):\n",
    "        out = dec_in\n",
    "        out = out + self.attn(out, out, out, mask)\n",
    "        out = out + self.attn_ffn(enc_out, enc_out, out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        block_size,\n",
    "        device,\n",
    "        embedding_dim,\n",
    "        nhead,\n",
    "        dim_feedforward,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, embedding_dim)\n",
    "        self.src_pos = nn.Embedding(block_size, embedding_dim)\n",
    "\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, embedding_dim)\n",
    "        self.tgt_pos = nn.Embedding(block_size, embedding_dim)\n",
    "\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(\n",
    "                    embedding_dim,\n",
    "                    nhead,\n",
    "                    dim_feedforward,\n",
    "                    dropout,\n",
    "                )\n",
    "                for _ in range(num_encoder_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.decoders = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    embedding_dim,\n",
    "                    nhead,\n",
    "                    dim_feedforward,\n",
    "                    dropout,\n",
    "                )\n",
    "                for _ in range(num_decoder_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(embedding_dim, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        _, srcT = src.shape\n",
    "        src_positions = torch.arange(srcT).unsqueeze(0).to(self.device)\n",
    "        src_out = self.src_emb(src) + self.src_pos(src_positions)\n",
    "        src_out = self.dropout(src_out)\n",
    "\n",
    "        for encoder in self.encoders:\n",
    "            src_out = encoder(src_out, src_out, src_out)\n",
    "\n",
    "        _, tgtT = tgt.shape\n",
    "        tgt_positions = torch.arange(tgtT).unsqueeze(0).to(self.device)\n",
    "        tgt_out = self.tgt_emb(tgt) + self.tgt_pos(tgt_positions)\n",
    "        tgt_out = self.dropout(tgt_out)\n",
    "\n",
    "        mask = generate_square_subsequent_mask(tgtT, self.device)\n",
    "\n",
    "        for decoder in self.decoders:\n",
    "            tgt_out = decoder(src_out, tgt_out, mask)\n",
    "\n",
    "        tgt_out = self.proj(tgt_out)\n",
    "        tgt_out = self.dropout(tgt_out)\n",
    "\n",
    "        return tgt_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_beyts(filename, split_files):\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines(keepends=True)\n",
    "\n",
    "    lines = [line.split(mesra_delimiter) for line in lines]\n",
    "    src = [line[0] for line in lines if len(line) == 2]\n",
    "    tgt = [mesra_delimiter + line[1] for line in lines if len(line) == 2]\n",
    "\n",
    "    with open(split_files[0], \"w\") as f:\n",
    "        f.write(beyt_delimiter.join(src))\n",
    "\n",
    "    with open(split_files[1], \"w\") as f:\n",
    "        f.write(\"\".join(tgt))\n",
    "\n",
    "\n",
    "def get_tokenizer(filenames):\n",
    "    tk = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tk.enable_padding(pad_id=3)\n",
    "    # tk.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "    tk.train(filenames, trainer)\n",
    "\n",
    "    return tk\n",
    "\n",
    "\n",
    "def split(data, train_ratio=0.8, val_ratio=0.1):\n",
    "    src, tgt = data\n",
    "    assert len(src) == len(tgt), \"expeted the same source and target sizes.\"\n",
    "\n",
    "    ntrain = int(train_ratio * len(src))\n",
    "    nval = int(val_ratio * len(src))\n",
    "    indices = torch.randperm(len(src))\n",
    "\n",
    "    train = (src[indices][:ntrain], tgt[indices][:ntrain])\n",
    "    val = (src[indices][ntrain : ntrain + nval], tgt[indices][ntrain : ntrain + nval])\n",
    "    test = (src[indices][ntrain + nval :], tgt[indices][ntrain + nval :])\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def get_batch(data, batch_size, device):\n",
    "    \"\"\"\n",
    "    Generates a batch of examples.\n",
    "    \"\"\"\n",
    "    src, tgt = data\n",
    "    assert len(src) == len(tgt), \"expeted the same source and target sizes.\"\n",
    "\n",
    "    indices = torch.randint(len(src), (batch_size,))\n",
    "\n",
    "    x = src[indices].to(device)\n",
    "    y = tgt[indices].to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_loss(logits, y, ignore_index):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss, given logits and labels.\n",
    "    \"\"\"\n",
    "    B, T, C = logits.shape\n",
    "    # F.cross_entropy expects size C, (B, C), or (B, C, ...)\n",
    "    # logits shape is (B, T, C), so we flatten the first two dimensions.\n",
    "    return F.cross_entropy(\n",
    "        logits.view(B * T, C), y.reshape(B * T), ignore_index=ignore_index\n",
    "    )\n",
    "\n",
    "\n",
    "def generate(first_mesra, tk, model, device):\n",
    "    \"\"\"\n",
    "    Generates second mesra.\n",
    "    \"\"\"\n",
    "    token_ids = tk.encode(first_mesra).ids\n",
    "    x = torch.tensor(token_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    y = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "    while True:\n",
    "        logits = model(x, y)\n",
    "        # only consider the last logit\n",
    "        logits = logits[:, -1, :]\n",
    "        score = F.softmax(logits, dim=-1)\n",
    "        next_token_id = score.multinomial(1)\n",
    "        if \"\\n\" in tk.id_to_token(next_token_id):\n",
    "            break\n",
    "        y = torch.cat((y, next_token_id), dim=1)\n",
    "\n",
    "    y = y.view(-1)\n",
    "    return \" \".join([tk.id_to_token(t) for t in y[1:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cpu\n",
      "\n",
      "\n",
      "\n",
      "vocab size: 30000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"running on {device}\")\n",
    "\n",
    "text_file = \"data/shahname.txt\"\n",
    "split_files = [\"data/shahname_src.txt\", \"data/shahname_tgt.txt\"]\n",
    "split_beyts(text_file, split_files)\n",
    "\n",
    "tk = get_tokenizer(split_files)\n",
    "vocab_size = tk.get_vocab_size()\n",
    "print(f\"vocab size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 754160\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    num_encoder_layers=params[\"num_encoder_layers\"],\n",
    "    num_decoder_layers=params[\"num_decoder_layers\"],\n",
    "    block_size=params[\"block_size\"],\n",
    "    device=device,\n",
    "    embedding_dim=params[\"embedding_dim\"],\n",
    "    nhead=params[\"nhead\"],\n",
    "    dim_feedforward=params[\"dim_feedforward\"],\n",
    "    dropout=params[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "num_params = sum([p.nelement() for p in model.parameters()])\n",
    "print(f\"model parameters: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model parameters: 754160\n",
      "\n",
      "first mesra: چو ترکان بدیدند کارجاسپ رفت\n",
      "epoch 0 / 10\n",
      "second mesra:\n",
      "\n",
      "epoch 1 / 10\n",
      "second mesra:\n",
      "هران \n",
      "epoch 2 / 10\n",
      "second mesra:\n",
      "کارداران  \tکشیده  \tبه نزد ب خوب گفتار  ما را ز  رخنهٔ  قر شیران ب\n",
      "epoch 3 / 10\n",
      "second mesra:\n",
      "خون بر کینهٔ  سع چو بند \n",
      "epoch 4 / 10\n",
      "second mesra:\n",
      "\tفرستاد بر  شاهزاد \tپس از مرگ  و بیداردل \n",
      "epoch 5 / 10\n",
      "second mesra:\n",
      "\tبرو بوم  یمار \n",
      "epoch 6 / 10\n",
      "second mesra:\n",
      "جامی ز  همی باش  شای گیا  خرد نیست  روزی دهان  اندر آید به  شنگل  شماس\n",
      "epoch 7 / 10\n",
      "second mesra:\n",
      "\tبدان سو \n",
      "epoch 8 / 10\n",
      "second mesra:\n",
      "کوب \tنجنبی دوزخ ب\n",
      "epoch 9 / 10\n",
      "second mesra:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10\n",
    ")\n",
    "\n",
    "if load_model:\n",
    "    state = torch.load(model_filename)\n",
    "\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    scheduler.load_state_dict(state[\"scheduler\"])\n",
    "\n",
    "num_params = sum([p.nelement() for p in model.parameters()])\n",
    "print(f\"\\nmodel parameters: {num_params}\")\n",
    "\n",
    "first_mesra = \"چو ترکان بدیدند کارجاسپ رفت\"\n",
    "print(f\"\\nfirst mesra: {first_mesra}\")\n",
    "\n",
    "with open(\"data/shahname_src.txt\") as f:\n",
    "    src_lines = f.read().splitlines()\n",
    "\n",
    "src_token_ids = torch.tensor(\n",
    "    [x.ids for x in tk.encode_batch(src_lines)], dtype=torch.long\n",
    ")\n",
    "\n",
    "with open(\"data/shahname_tgt.txt\") as f:\n",
    "    tgt_lines = f.read().splitlines(keepends=True)\n",
    "\n",
    "tgt_token_ids = torch.tensor(\n",
    "    [x.ids for x in tk.encode_batch(tgt_lines)], dtype=torch.long\n",
    ")\n",
    "\n",
    "train, val, _ = split((src_token_ids, tgt_token_ids), 0.9, 0.1)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    print(f\"epoch {epoch} / {params['epochs']}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        second_mesra = generate(first_mesra, tk, model, device)\n",
    "        print(f\"second mesra:\\n{second_mesra}\")\n",
    "\n",
    "        src, tgt = get_batch(val, params[\"batch_size\"], device)\n",
    "\n",
    "        logits = model(src, tgt[:, :-1])\n",
    "        vloss = get_loss(logits, tgt[:, 1:], ignore_index=tk.token_to_id(\"[PAD]\"))\n",
    "        val_losses.append(vloss.item())\n",
    "\n",
    "    model.train()\n",
    "    src, tgt = get_batch(train, params[\"batch_size\"], device)\n",
    "\n",
    "    logits = model(src, tgt[:, :-1])\n",
    "    loss = get_loss(logits, tgt[:, 1:], ignore_index=tk.token_to_id(\"[PAD]\"))\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, model_filename)\n",
    "\n",
    "    scheduler.step(train_losses[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
